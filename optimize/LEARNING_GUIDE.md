# PPO å¼ºåŒ–å­¦ä¹ é¡¹ç›®å®Œæ•´å­¦ä¹ æŒ‡å—

> **ç›®æ ‡**ï¼šè®©ä½ ä» 0 ç†è§£åˆ° 100%ã€‚è¿™ä»½æ–‡æ¡£ä¼šç”¨æœ€ç®€æ´çš„ä¸­æ–‡é€è¡Œè®²è§£æ¯ä¸€éƒ¨åˆ†ä»£ç ã€‚

---

## ğŸ“š ç›®å½•

1. [é¡¹ç›®å…¨æ™¯å›¾](#é¡¹ç›®å…¨æ™¯å›¾)
2. [æ ¸å¿ƒæ¦‚å¿µé€Ÿæˆ](#æ ¸å¿ƒæ¦‚å¿µé€Ÿæˆ)
3. [é…ç½®æ–‡ä»¶è¯¦è§£](#é…ç½®æ–‡ä»¶è¯¦è§£)
4. [ç¥ç»ç½‘ç»œè®¾è®¡](#ç¥ç»ç½‘ç»œè®¾è®¡)
5. [è®­ç»ƒæµç¨‹è¯¦è§£](#è®­ç»ƒæµç¨‹è¯¦è§£)
6. [PPO ç®—æ³•æ•°å­¦åŸç†ä¸ä»£ç å¯¹åº”](#ppo-ç®—æ³•æ•°å­¦åŸç†ä¸ä»£ç å¯¹åº”)
7. [æ”¹è¿›æ–¹å‘æŒ‡å—](#æ”¹è¿›æ–¹å‘æŒ‡å—)

---

## é¡¹ç›®å…¨æ™¯å›¾

### ä½ å°†çœ‹åˆ°çš„æ‰§è¡Œæµç¨‹å›¾

```
å‘½ä»¤è¡Œè¾“å…¥
    â†“
config_ppo.yaml (è¯»å–é…ç½®)
    â†“
ppo_discrete.py main() å‡½æ•°
    â†“
make_train(config) (æ‰“åŒ…è®­ç»ƒå‡½æ•°)
    â†“
jax.vmap (å¹¶è¡ŒåŒ–ï¼šå¤šä¸ªç§å­åŒæ—¶è®­ç»ƒ)
    â†“
jax.jit (ç¼–è¯‘ä¼˜åŒ–ï¼šä»£ç è¢«è½¬æˆè¶…å¿«é€Ÿçš„æœºå™¨ç )
    â†“
è®­ç»ƒå¾ªç¯å¼€å§‹
  â”œâ”€ _train_loop (å¤–å±‚å¾ªç¯ï¼šnum_updates æ¬¡è¿­ä»£)
  â”‚  â””â”€ _env_step (å†…å±‚å¾ªç¯ï¼šä¸ç¯å¢ƒäº¤äº’ num_steps æ¬¡)
  â”‚     â”œâ”€ env.reset (ç¯å¢ƒé‡ç½®)
  â”‚     â”œâ”€ network.apply (ç¥ç»ç½‘ç»œå‰å‘æ¨ç†)
  â”‚     â””â”€ env.step (ä¸ç¯å¢ƒäº¤äº’ä¸€æ­¥)
  â”‚
  â”œâ”€ è®¡ç®— GAE (ä¼˜åŠ¿ä¼°è®¡)
  â”‚
  â”œâ”€ _update_epoch (æ›´æ–°ç¥ç»ç½‘ç»œå‚æ•°)
  â”‚  â””â”€ _update_minibatch (å¤„ç†ä¸€ä¸ªå°æ‰¹æ¬¡)
  â”‚     â”œâ”€ _loss (è®¡ç®—æŸå¤±)
  â”‚     â”œâ”€ jax.value_and_grad (è®¡ç®—æ¢¯åº¦)
  â”‚     â””â”€ apply_gradients (æ›´æ–°å‚æ•° â† Adam ä¼˜åŒ–å™¨åœ¨è¿™é‡Œå·¥ä½œ)
  â”‚
  â””â”€ æ—¥å¿—å›è°ƒ (é€šè¿‡ Wandb è®°å½•æŒ‡æ ‡)
    
æ‰€æœ‰ç»“æœæ±‡æ€»åˆ° Wandb ä»ªè¡¨æ¿
```

### ä¸€å¥è¯ç†è§£æ¯ä¸ªæ–‡ä»¶

| æ–‡ä»¶ | ä½œç”¨ | éš¾åº¦ |
|------|------|------|
| `config_ppo.yaml` | å®éªŒé…ç½®ï¼ˆè¶…å‚æ•°ï¼‰ | â­ |
| `networks/mlp.py` | ç¥ç»ç½‘ç»œç»“æ„å®šä¹‰ | â­â­ |
| `ppo_discrete.py` | è®­ç»ƒé€»è¾‘çš„å®Œæ•´å®ç° | â­â­â­â­ |
| `utils/wandb_multilogger.py` | æ—¥å¿—ç®¡ç†ï¼ˆåå°è¿›ç¨‹ï¼‰ | â­â­ |
| `utils/jax_utils.py` | å·¥å…·å‡½æ•° | â­ |

---

## æ ¸å¿ƒæ¦‚å¿µé€Ÿæˆ

### 1ï¸âƒ£ ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ

æƒ³è±¡ä½ æ˜¯ä¸€ä¸ªå°æœºå™¨äººåœ¨å­¦èµ°è·¯ï¼š

```
ç°åœ¨çœ‹åˆ°çš„ç”»é¢ (è§‚æµ‹)
    â†“
å¤§è„‘æ€è€ƒï¼šå‘å‰èµ°è¿˜æ˜¯å‘åèµ°ï¼Ÿ(ç­–ç•¥)
    â†“
é€‰æ‹©ï¼šå‘å‰èµ°ä¸€æ­¥ (åŠ¨ä½œ)
    â†“
ç¯å¢ƒç»™åé¦ˆï¼š+1 åˆ† (å¥–åŠ±)
    â†“
å¤§è„‘å­¦åˆ°ï¼šåœ¨è¿™ä¸ªåœºæ™¯ä¸‹å‘å‰èµ°æ˜¯å¥½çš„ï¼
    â†“
é‡å¤ä¸Šé¢çš„è¿‡ç¨‹ï¼Œä¸€ç›´æ”¹è¿›
```

åœ¨æˆ‘ä»¬çš„ä»£ç é‡Œï¼š
- **è§‚æµ‹ (obs)**ï¼šå½“å‰æ¸¸æˆç”»é¢/çŠ¶æ€
- **åŠ¨ä½œ (action)**ï¼šAI å†³å®šåšä»€ä¹ˆï¼ˆæ¯”å¦‚"å‘å·¦"æˆ–"å‘å³"ï¼‰
- **å¥–åŠ± (reward)**ï¼šç¯å¢ƒç»™çš„åˆ†æ•°åé¦ˆ
- **å¤§è„‘**ï¼šå°±æ˜¯ `ActorCriticDiscrete` ç¥ç»ç½‘ç»œ

### 2ï¸âƒ£ ä»€ä¹ˆæ˜¯ PPOï¼ˆç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼‰ï¼Ÿ

PPO æ˜¯ä¸€ç§å¾ˆèªæ˜çš„å­¦ä¹ æ–¹æ³•ã€‚å®ƒåˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼š

- **Actorï¼ˆæ¼”å‘˜ï¼‰**ï¼šå†³å®šé‡‡å–ä»€ä¹ˆåŠ¨ä½œã€‚"æˆ‘è§‰å¾—åº”è¯¥å‘å‰èµ°"
- **Criticï¼ˆè¯„è®ºå®¶ï¼‰**ï¼šè¯„ä¼°å½“å‰çŠ¶æ€æœ‰å¤šå¥½ã€‚"è¿™ä¸ªçŠ¶æ€å€¼ 5 åˆ†"

PPO çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**ä¸è¦ä¸€æ¬¡æ”¹å˜å¤ªå¤š**ã€‚

```
æ—§ç­–ç•¥çš„è¡Œä¸º vs æ–°ç­–ç•¥çš„è¡Œä¸º
  â†“
å®ƒä»¬çš„æ¯”ä¾‹ä¸åº”è¯¥å·®å¤ªå¤šï¼ˆå¦åˆ™å­¦ä¹ å°±"è·³è¿‡äº†"æ­£ç¡®ç­”æ¡ˆï¼‰
  â†“
é€šè¿‡ Clipped Surrogate Objectiveï¼ˆè£å‰ªæ›¿ä»£ç›®æ ‡ï¼‰æ¥æ§åˆ¶
```

### 3ï¸âƒ£ ä»€ä¹ˆæ˜¯ Adam ä¼˜åŒ–å™¨ï¼Ÿ

Adam æ˜¯"èªæ˜çš„è°ƒå‚å¤§å¸ˆ"ã€‚å½“ä½ ç®—å‡ºæ¢¯åº¦ï¼ˆå‚æ•°åº”è¯¥æ€ä¹ˆæ”¹ï¼‰åï¼ŒAdam ä¼šï¼š

```
è®°ä½æœ€è¿‘æ¢¯åº¦çš„å¹³å‡æ–¹å‘ï¼ˆä¸€é˜¶çŸ©ï¼‰
è®°ä½æœ€è¿‘æ¢¯åº¦å˜åŒ–çš„å¼ºåº¦ï¼ˆäºŒé˜¶çŸ©ï¼‰
  â†“
ç»¼åˆè€ƒè™‘ï¼šæ—¢è¦æ²¿ç€æ¢¯åº¦æ–¹å‘ï¼Œä¹Ÿè¦è€ƒè™‘æ¢¯åº¦çš„"ç¨³å®šæ€§"
  â†“
æœ€ç»ˆå†³å®šï¼šè¿™æ¬¡åº”è¯¥æ”¹å˜å¤šå°‘
```

- **beta_1** (é»˜è®¤ 0.9)ï¼šå¯¹å†å²æ¢¯åº¦æ–¹å‘çš„"è®°å¿†"æœ‰å¤šå¼º
- **beta_2** (é»˜è®¤ 0.999)ï¼šå¯¹æ¢¯åº¦æ³¢åŠ¨çš„"è®°å¿†"æœ‰å¤šå¼º

### 4ï¸âƒ£ ä»€ä¹ˆæ˜¯ JAXï¼Ÿ

JAX å°±æ˜¯"å¸¦è¶…èƒ½åŠ›çš„ NumPy"ï¼š

```
NumPy              â†’    JAX
åœ¨ CPU ä¸Šè¿è¡Œ      â†’    åœ¨ GPU/TPU ä¸Šè¿è¡Œï¼ˆå¿«å¾—å¤šï¼ï¼‰
æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦       â†’    è‡ªåŠ¨å¾®åˆ†ï¼ˆjax.gradï¼‰
æ— æ³•ç¼–è¯‘           â†’    JIT ç¼–è¯‘ï¼ˆjax.jitï¼Œå¿«å¾—å¤šï¼ï¼‰
æ— æ³•å‘é‡åŒ–         â†’    è‡ªåŠ¨å‘é‡åŒ–ï¼ˆjax.vmapï¼Œè½»æ¾å¹¶è¡ŒåŒ–ï¼‰
```

---

## é…ç½®æ–‡ä»¶è¯¦è§£

### `config_ppo.yaml` é€è¡Œè§£é‡Š

```yaml
# ============ è®­ç»ƒå‚æ•° ============

# æ€»å…±è¦è®­ç»ƒå¤šå°‘æ­¥ï¼Ÿ(å•ä½ï¼šç¯å¢ƒäº¤äº’)
# ä¾‹å¦‚ï¼š2e6 = 2,000,000 æ­¥
# ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒé€šå¸¸éœ€è¦å‡ ç™¾ä¸‡åˆ°å‡ åƒä¸‡æ­¥æ‰èƒ½çœ‹åˆ°æ•ˆæœ
total_timesteps: 2e6

# åŒæ—¶è¿è¡Œå¤šå°‘ä¸ªæ¸¸æˆç¯å¢ƒï¼Ÿ
# æ•°å­—è¶Šå¤§ï¼Œä¸€æ¬¡æ”¶é›†ç»éªŒè¶Šå¤šï¼Œä½†ä¼šå ç”¨æ›´å¤šå†…å­˜å’Œ GPU
# 16 ä¸ªç¯å¢ƒæ˜¯ä¸ªå¹³è¡¡ç‚¹
num_envs: 16

# æ¯æ¬¡æ”¶é›†å¤šå°‘æ­¥çš„ç»éªŒï¼Ÿç„¶åå†è¿›è¡Œä¸€æ¬¡ç½‘ç»œæ›´æ–°
# num_steps è¶Šå¤§ï¼šæ›´ç¨³å®šä½†è®¡ç®—æ…¢
# num_steps è¶Šå°ï¼šæ›´å¿«ä½†å¯èƒ½ä¸ç¨³å®š
num_steps: 128

# å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰ï¼šæ¢¯åº¦æ›´æ–°æ—¶çš„"æ­¥é•¿"
# lr å¤ªå¤§ï¼šå­¦ä¹ ä¸ç¨³å®šï¼Œå¯èƒ½å‘æ•£
# lr å¤ªå°ï¼šå­¦ä¹ å¤ªæ…¢ï¼Œå®¹æ˜“å¡åœ¨å±€éƒ¨æœ€ä¼˜
# 4e-3 = 0.004ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„èµ·ç‚¹
"lr": 4e-3

# é€‰æ‹©ä¼˜åŒ–å™¨ç±»å‹ï¼šadam, rmsprop, sgd
# Adam æœ€å¸¸ç”¨ï¼ˆæˆ‘ä»¬çš„é¡¹ç›®å°±åœ¨ç ”ç©¶ Adamï¼‰
# RMSprop æ˜¯ Adam çš„ç®€åŒ–ç‰ˆ
# SGD æ˜¯æœ€åŸºç¡€çš„
"optimizer": "adam"

# æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡ï¼Ÿ
# trueï¼šéšç€è®­ç»ƒè¿›è¡Œï¼Œå­¦ä¹ ç‡ä¼šé€æ­¥é™ä½
# è¿™æ ·å‰æœŸå­¦å¾—å¿«ï¼ŒåæœŸå­¦å¾—ç¨³å®š
"anneal_lr": True

# Adam ä¼˜åŒ–å™¨çš„å‚æ•° â† è¿™æ­£æ˜¯ä½ çš„ç ”ç©¶é¡¹ç›®è¦è°ƒæŸ¥çš„ï¼
"beta_1": 0.9      # ä¸€é˜¶çŸ©ï¼ˆæ¢¯åº¦å¹³å‡ï¼‰çš„æŒ‡æ•°è¡°å‡ç‡
"beta_2": 0.999    # äºŒé˜¶çŸ©ï¼ˆæ¢¯åº¦å¹³æ–¹å¹³å‡ï¼‰çš„æŒ‡æ•°è¡°å‡ç‡

# ============ PPO ç®—æ³•å‚æ•° ============

# ä¸€ä¸ªæ•°æ®è¢«ç”¨æ¥æ›´æ–°ç½‘ç»œå¤šå°‘æ¬¡ï¼Ÿ
# è¶Šå¤šï¼šæ•°æ®è¢«å……åˆ†åˆ©ç”¨ä½†å¯èƒ½è¿‡æ‹Ÿåˆ
# è¶Šå°‘ï¼šæ¯ä¸ªæ•°æ®è¢«å……åˆ†åˆ©ç”¨å¯èƒ½ä¸å¤Ÿ
"update_epochs": 2

# æ¢¯åº¦ä¸‹é™æ—¶åˆ†æˆå¤šå°‘ä¸ªå°æ‰¹æ¬¡ï¼Ÿ
# å¦‚æœ num_envs=16, num_steps=128ï¼Œæ€»å…±æœ‰ 16*128=2048 æ¡æ•°æ®
# åˆ†æˆ 4 ä¸ªå°æ‰¹æ¬¡ï¼Œæ¯ä¸ª 512 æ¡æ•°æ®
# å°æ‰¹æ¬¡è¶Šå°ï¼šæ¢¯åº¦ä¼°è®¡å™ªå£°è¶Šå¤§ï¼›è¶Šå¤§ï¼šå†…å­˜éœ€æ±‚è¶Šå¤§
"num_minibatches": 4

# æŠ˜æ‰£å› å­ï¼ˆDiscount Factorï¼‰ï¼šå¤šå°‘æ­¥ä¹‹åçš„å¥–åŠ±ä»·å€¼è¡°å‡ä¸º 0ï¼Ÿ
# gamma=0.99ï¼šè¡¨ç¤º 100 æ­¥ä¹‹å¤–çš„å¥–åŠ±å‡ ä¹æ— å…³ç´§è¦
# gamma è¶Šæ¥è¿‘ 1.0ï¼šæ›´è¿œçš„æœªæ¥å¥–åŠ±æ›´é‡è¦ï¼ˆè§†é‡æ›´é•¿ï¼‰
# gamma è¶Šå°ï¼šåªå…³å¿ƒçœ¼å‰åˆ©ç›Š
"gamma": 0.99

# GAE Lambdaï¼šåœ¨è®¡ç®—ä¼˜åŠ¿ä¼°è®¡æ—¶çš„å¹³æ»‘å‚æ•°
# è¿™ä¸ªå€¼è¶Šå°ï¼šä¼°è®¡è¶Šä¾èµ–å½“å‰çš„ Critic ç½‘ç»œï¼ˆå¯èƒ½æœ‰åå·®ä½†ä½æ–¹å·®ï¼‰
# è¿™ä¸ªå€¼è¶Šå¤§ï¼šä¼°è®¡è¶Šä¾èµ–å®é™…ç»éªŒï¼ˆæ–¹å·®å¤§ä½†æ— åï¼‰
"gae_lambda": 0.95

# PPO çš„æ ¸å¿ƒï¼šè£å‰ªèŒƒå›´
# å¦‚æœæ–°ç­–ç•¥ä¸æ—§ç­–ç•¥çš„æ¯”ä¾‹è¶…è¿‡è¿™ä¸ªèŒƒå›´ï¼Œä¼šè¢«"è£å‰ª"ï¼ˆé˜»æ­¢çªç„¶å¤§å˜åŒ–ï¼‰
# clip_eps=0.2 è¡¨ç¤º Â±20% çš„èŒƒå›´
# æ¯”ä¾‹åœ¨ [0.8, 1.2] èŒƒå›´å†…æ‰ä¼šå®é™…å½±å“æ¢¯åº¦
"clip_eps": 0.2

# ç†µå¥–åŠ±ç³»æ•°ï¼ˆEntropy Coefficientï¼‰
# ç†µ = ç­–ç•¥éšæœºæ€§çš„åº¦é‡
# ent_coef è¶Šå¤§ï¼šAI æ›´éšæœºï¼Œæ¢ç´¢èƒ½åŠ›å¼ºä½†å®¹æ˜“ä¸ç¨³å®š
# ent_coef è¶Šå°ï¼šAI æ›´ç¡®å®šï¼Œå®¹æ˜“è¿‡æ—©æ”¶æ•›
"ent_coef": 0.003

# ä»·å€¼æŸå¤±ç³»æ•°ï¼ˆValue Function Coefficientï¼‰
# è¿™ä¸ªç³»æ•°ä¹˜ä»¥ Critic çš„æŸå¤±ï¼Œæ§åˆ¶ Critic ç½‘ç»œçš„å­¦ä¹ å¼ºåº¦
# é€šå¸¸è®¾ä¸º 0.5
"vf_coef": 0.5

# æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰
# å¦‚æœæ¢¯åº¦èŒƒæ•°è¶…è¿‡è¿™ä¸ªå€¼ï¼Œå°±è¢«ç¼©æ”¾ä¸‹æ¥
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆçªç„¶éå¸¸å¤§çš„æ›´æ–°å¯¼è‡´è®­ç»ƒå´©æºƒï¼‰
"max_grad_norm": 1.0

# æ¿€æ´»å‡½æ•°ï¼šrelu æˆ– tanh
# reluï¼šf(x) = max(0, x)ï¼Œè®¡ç®—å¿«ï¼Œç°ä»£æ›´å¸¸ç”¨
# tanhï¼šf(x) = tanh(x)ï¼ŒèŒƒå›´ [-1, 1]ï¼Œè¾ƒæ—§ä½†æœ‰æ—¶æ›´ç¨³å®š
"activation": "relu"

# ============ ç¯å¢ƒå’Œç§å­ ============

# ä½¿ç”¨å“ªä¸ªæ¸¸æˆç¯å¢ƒï¼Ÿ
# MountainCar-v0ï¼šä¸€ä¸ªç®€å•çš„ç™»å±±è½¦æ¸¸æˆ
# è¿˜æœ‰å…¶ä»–ç¯å¢ƒå¦‚ CartPole-v1, LunarLander-v2 ç­‰
env_name: "MountainCar-v0"

# éšæœºç§å­ï¼šæ§åˆ¶éšæœºæ•°çš„åˆå€¼ï¼Œä½¿å®éªŒå¯é‡å¤
seed: 0

# è¿è¡Œå¤šå°‘ä¸ªç‹¬ç«‹çš„ seedï¼Ÿï¼ˆåš N æ¬¡å®éªŒå–å¹³å‡ï¼‰
# è¿™æ ·å¯ä»¥çœ‹å‡ºç®—æ³•çš„ç¨³å®šæ€§
num_seeds: 10

# ============ Wandb æ—¥å¿—é…ç½® ============

# é¡¹ç›®åç§°ï¼šWandb ä¸Šåˆ›å»ºå“ªä¸ªé¡¹ç›®ï¼Ÿ
project: optimize

# ä½œä¸šç±»å‹æ ‡ç­¾
job_type: ppo

# æ¨¡å¼ï¼šonlineï¼ˆä¸Šä¼ åˆ°ç½‘ç»œï¼‰/ disabledï¼ˆæœ¬åœ°è¿è¡Œï¼‰
wandb_mode: "online"
```

### ç†è§£è¶…å‚æ•°ä¹‹é—´çš„å…³ç³»

```
total_timesteps = num_updates Ã— num_steps Ã— num_envs

ä¾‹å¦‚ï¼š
2,000,000 = num_updates Ã— 128 Ã— 16
num_updates = 2,000,000 / 128 / 16 = 977.5 â‰ˆ 977 æ¬¡å¤§å¾ªç¯

æ¯æ¬¡å¤§å¾ªç¯ï¼š
  1. æ”¶é›†ï¼šä¸ 16 ä¸ªç¯å¢ƒäº¤äº’ 128 æ­¥ = 2048 æ¡ç»éªŒ
  2. æ›´æ–°ï¼šå¯¹è¿™ 2048 æ¡æ•°æ®æ›´æ–° 2 ä¸ª epochï¼ˆÃ—2 æ¬¡å¾ªç¯ï¼‰
           æ¯ä¸ª epoch åˆ†æˆ 4 ä¸ª minibatchï¼ˆ512 æ¡æ•°æ®/batchï¼‰
```

---

## ç¥ç»ç½‘ç»œè®¾è®¡

### `networks/mlp.py` è¯¦è§£

```python
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  ç½‘ç»œç»“æ„å›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰                           â•‘
# â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
# â•‘                                                â•‘
# â•‘  è¾“å…¥è§‚æµ‹ (è§‚æµ‹ç»´åº¦)                            â•‘
# â•‘       â†“                                         â•‘
# â•‘  å…±äº«å±‚ 1: Dense(64) + ReLU                     â•‘
# â•‘       â†“                                         â•‘
# â•‘  å…±äº«å±‚ 2: Dense(64) + ReLU                     â•‘
# â•‘       â†™          â†˜                             â•‘
# â•‘  Actor åˆ†æ”¯    Critic åˆ†æ”¯                     â•‘
# â•‘       â†“              â†“                         â•‘
# â•‘ Dense(action_dim)  Dense(1)                    â•‘
# â•‘       â†“              â†“                         â•‘
# â•‘   Policy åˆ†å¸ƒ      Value æ ‡é‡                   â•‘
# â•‘  (Categorical)                                 â•‘
# â•‘                                                â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ**

1. **ä¸¤ä¸ªåˆ†æ”¯ï¼ˆActor-Criticï¼‰**
   - Actorï¼šå†³å®šé‡‡å–å“ªä¸ªåŠ¨ä½œçš„æ¦‚ç‡
   - Criticï¼šè¯„ä¼°å½“å‰çŠ¶æ€çš„"å¥½åç¨‹åº¦"

2. **å…±äº«åº•å±‚ï¼ˆéšè—å±‚ 1 å’Œ 2ï¼‰**
   - è®©ä¸¤ä¸ªç½‘ç»œå­¦åˆ°ç›¸åŒçš„"ç‰¹å¾è¡¨ç¤º"
   - èŠ‚çœå‚æ•°ï¼ŒåŠ é€Ÿå­¦ä¹ 

3. **éšè—å±‚å¤§å° = 64**
   - è¿™æ˜¯æœ€å¸¸ç”¨çš„å¤§å°
   - å¤ªå°ï¼ˆ8ï¼‰ï¼šå®¹é‡ä¸è¶³ï¼Œå­¦ä¸ä¼š
   - å¤ªå¤§ï¼ˆ512ï¼‰ï¼šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè®¡ç®—æ…¢

---

## è®­ç»ƒæµç¨‹è¯¦è§£

### ç¬¬ 1 å±‚ï¼šåˆå§‹åŒ–

```python
# ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºç¯å¢ƒ
env, env_params = gymnax.make("MountainCar-v0")
# envï¼šç¯å¢ƒå¯¹è±¡ï¼Œæœ‰ .reset() å’Œ .step() æ–¹æ³•
# env_paramsï¼šç¯å¢ƒçš„å›ºå®šå‚æ•°ï¼ˆå¦‚é‡åŠ›åŠ é€Ÿåº¦ç­‰ï¼‰

# ç¬¬äºŒæ­¥ï¼šåˆ›å»ºç¥ç»ç½‘ç»œ
network = ActorCriticDiscrete(action_dim=env.num_actions, activation="relu")
# action_dimï¼šç¯å¢ƒçš„åŠ¨ä½œæ•°é‡ï¼ˆMountainCar æœ‰ 3 ä¸ªåŠ¨ä½œï¼šå·¦ã€ä¸åŠ¨ã€å³ï¼‰

# ç¬¬ä¸‰æ­¥ï¼šåˆå§‹åŒ–ç½‘ç»œå‚æ•°
rng = jax.random.PRNGKey(0)  # åˆ›å»ºéšæœºæ•°ç”Ÿæˆå™¨
init_x = jnp.zeros(obs.shape)  # å‡è¾“å…¥ï¼ˆç”¨æ¥æ¨æ–­ç½‘ç»œå½¢çŠ¶ï¼‰
network_params = network.init(rng, init_x)  # éšæœºåˆå§‹åŒ–æ‰€æœ‰æƒé‡å’Œåå·®
# network_params æ˜¯ä¸€ä¸ªåµŒå¥—çš„å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰æƒé‡

# ç¬¬å››æ­¥ï¼šåˆ›å»ºä¼˜åŒ–å™¨
tx = optax.chain(
    optax.clip_by_global_norm(1.0),  # æ¢¯åº¦è£å‰ª
    optax.adam(learning_rate=0.004, b1=0.9, b2=0.999),  # Adam ä¼˜åŒ–å™¨
)
# optax.chainï¼šä¾æ¬¡åº”ç”¨å¤šä¸ªä¼˜åŒ–æ“ä½œ

# ç¬¬äº”æ­¥ï¼šåˆ›å»ºè®­ç»ƒçŠ¶æ€
train_state = TrainState.create(
    apply_fn=network.apply,  # ç½‘ç»œå‰å‘å‡½æ•°
    params=network_params,    # åˆå§‹ç½‘ç»œå‚æ•°
    tx=tx,                    # ä¼˜åŒ–å™¨
)
# train_state æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…å«ï¼š
#   - paramsï¼šå½“å‰ç½‘ç»œå‚æ•°
#   - opt_stateï¼šä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdam ä¼šè®°ä½è¿‡å»æ¢¯åº¦çš„ä¿¡æ¯ï¼‰
```

### ç¬¬ 2 å±‚ï¼šæ•°æ®æ”¶é›†å¾ªç¯

```python
# ä¼ªä»£ç ï¼šæ¯ä¸ª update å‘¨æœŸ

for update in range(num_updates):  # 977 æ¬¡
    
    # â”€â”€â”€â”€ ç¬¬ä¸€é˜¶æ®µï¼šä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ç»éªŒ â”€â”€â”€â”€
    
    transitions = []  # å­˜å‚¨æ‰€æœ‰ç»éªŒ
    
    for step in range(num_steps):  # 128 æ­¥
        
        # 1. è·å–ç½‘ç»œçš„å½“å‰å†³ç­–
        pi, value = network.apply(train_state.params, obs)
        # pi: ç­–ç•¥åˆ†å¸ƒï¼ˆCategoricalï¼‰
        # value: Critic çš„è¯„ä¼°ï¼ˆæ ‡é‡ï¼‰
        
        # 2. é‡‡æ ·åŠ¨ä½œ
        action = pi.sample(seed=rng)
        # action: ä»åˆ†å¸ƒä¸­éšæœºé‡‡æ ·ï¼ˆå¦‚ 0ã€1 æˆ– 2ï¼‰
        
        # 3. è®¡ç®—è¯¥åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
        log_prob = pi.log_prob(action)
        # log_prob: ç”¨äºåé¢çš„ PPO æŸå¤±è®¡ç®—
        
        # 4. åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, done, info = env.step(action)
        # obs: æ–°çš„è§‚æµ‹ï¼ˆæ¸¸æˆç”»é¢ï¼‰
        # reward: è·å¾—çš„åˆ†æ•°
        # done: æ˜¯å¦æ¸¸æˆç»“æŸ
        
        # 5. å­˜å‚¨è¿™ä¸€æ­¥çš„ç»éªŒ
        transitions.append({
            'obs': obs,
            'action': action,
            'log_prob': log_prob,
            'reward': reward,
            'value': value,
            'done': done,
        })
    
    # transitions ç°åœ¨åŒ…å« 128 æ­¥çš„ç»éªŒ
    # å½¢çŠ¶ï¼š(128 æ­¥, 16 ä¸ªå¹¶è¡Œç¯å¢ƒ, ...)
```

### ç¬¬ 3 å±‚ï¼šè®¡ç®—ä¼˜åŠ¿ï¼ˆGAEï¼‰

```python
# GAE = Generalized Advantage Estimationï¼ˆå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼‰

# åŸºæœ¬æ€æƒ³ï¼š
# ä¸€ä¸ªçŠ¶æ€æœ‰å¤šå¥½ = å®é™…è·å¾—çš„å¥–åŠ± - ç½‘ç»œé¢„æµ‹çš„ä»·å€¼
#              = è¶…å‡ºé¢„æœŸçš„éƒ¨åˆ†
#              = "æƒŠå–œåº¦"

# ä¾‹å­ï¼š
# çœŸå®å›æŠ¥ = 10 åˆ†
# ç½‘ç»œé¢„æµ‹ = 8 åˆ†
# ä¼˜åŠ¿ = 10 - 8 = 2 åˆ†ï¼ˆè¶…å‡ºé¢„æœŸï¼‰

# ä»£ç ä¸­ï¼š
advantages, targets = _calculate_gae(traj_batch, last_value)
# advantages: æ¯ä¸€æ­¥çš„"æƒŠå–œåº¦"ï¼ˆå½¢çŠ¶åŒ rewardsï¼‰
# targets: æ¯ä¸€æ­¥çš„ç›®æ ‡å›æŠ¥ï¼ˆç”¨æ¥è®­ç»ƒ Criticï¼‰

# advantages ç”¨å¤„ï¼š
# - å¤§çš„æ­£å€¼ï¼šè¿™ä¸€æ­¥åšå¾—å¾ˆå¥½ï¼Œåº”è¯¥å¢åŠ è¯¥åŠ¨ä½œçš„æ¦‚ç‡
# - å°çš„è´Ÿå€¼ï¼šè¿™ä¸€æ­¥åšå¾—å¾ˆå·®ï¼Œåº”è¯¥å‡å°‘è¯¥åŠ¨ä½œçš„æ¦‚ç‡
```

### ç¬¬ 4 å±‚ï¼šç½‘ç»œæ›´æ–°ï¼ˆæ ¸å¿ƒç®—æ³•ï¼‰

```python
# â”€â”€â”€â”€ ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨æ”¶é›†çš„ç»éªŒæ¥æ›´æ–°ç½‘ç»œ â”€â”€â”€â”€

# ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡æ•°æ®
# éšæœºæ‰“ä¹± 16 ä¸ªç¯å¢ƒçš„æ•°æ®é¡ºåº
permutation = jax.random.permutation(rng, num_envs)
batch = (traj_batch, advantages, targets)
shuffled_batch = jax.tree.map(lambda x: jnp.take(x, permutation, axis=1), batch)

# ç¬¬äºŒæ­¥ï¼šåˆ†æˆ minibatch
# æŠŠ 2048 æ¡æ•°æ®åˆ†æˆ 4 ç»„ï¼Œæ¯ç»„ 512 æ¡
minibatches = split_into_minibatches(shuffled_batch, num_minibatches=4)

# ç¬¬ä¸‰æ­¥ï¼šå¯¹æ¯ä¸ª minibatch æ›´æ–°ç½‘ç»œ 2 æ¬¡ï¼ˆupdate_epochs=2ï¼‰
for epoch in range(update_epochs):
    for minibatch in minibatches:
        
        # â”€â”€â”€â”€ æŸå¤±è®¡ç®— â”€â”€â”€â”€
        
        # 1. é‡æ–°è®¡ç®—è¯¥ minibatch çš„ç­–ç•¥å’Œä»·å€¼
        pi, value_new = network.apply(train_state.params, minibatch.obs)
        log_prob_new = pi.log_prob(minibatch.action)
        
        # 2. è®¡ç®— PPO æŸå¤±ï¼ˆä¸»è¦ï¼‰
        ratio = exp(log_prob_new - minibatch.log_prob_old)
        # ratio: æ–°ç­–ç•¥ä¸æ—§ç­–ç•¥çš„æ¯”ç‡
        # å¦‚æœ ratio = 1.0ï¼šä¸¤ä¸ªç­–ç•¥ç›¸åŒ
        # å¦‚æœ ratio = 1.2ï¼šæ–°ç­–ç•¥åšè¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡é«˜ 20%
        
        advantage = minibatch.advantage
        advantage_normalized = (advantage - mean) / std  # æ ‡å‡†åŒ–ä¼˜åŠ¿
        
        # PPO çš„æ ¸å¿ƒï¼šæœ€å°åŒ–ä»¥ä¸‹ä¸¤ä¸ªå€¼çš„è¾ƒå°è€…
        loss_actor_unclipped = ratio * advantage_normalized
        loss_actor_clipped = clip(ratio, 0.8, 1.2) * advantage_normalized
        loss_actor = -min(loss_actor_unclipped, loss_actor_clipped).mean()
        
        # 3. è®¡ç®— Critic æŸå¤±
        value_loss = (value_new - minibatch.target) ** 2
        value_loss = 0.5 * value_loss.mean()
        
        # 4. è®¡ç®—ç†µæŸå¤±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        entropy = pi.entropy().mean()
        
        # 5. æ€»æŸå¤± = Actor æŸå¤± + 0.5 Ã— Critic æŸå¤± - 0.003 Ã— ç†µ
        total_loss = loss_actor + 0.5 * value_loss - 0.003 * entropy
        
        # â”€â”€â”€â”€ æ¢¯åº¦è®¡ç®—ä¸æ›´æ–°ï¼ˆAdam ä¼˜åŒ–å™¨ï¼‰ â”€â”€â”€â”€
        
        # 6. è®¡ç®—æ¢¯åº¦
        gradients = jax.grad(total_loss)(train_state.params)
        # gradients: å¯¹æ¯ä¸ªå‚æ•°çš„åå¯¼æ•°
        
        # 7. æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        gradients = clip_by_global_norm(gradients, max_norm=1.0)
        
        # 8. Adam ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°
        train_state = train_state.apply_gradients(grads=gradients)
        # è¿™é‡Œ Adam ä½¿ç”¨ beta_1=0.9 å’Œ beta_2=0.999 æ¥å†³å®šå‚æ•°æ”¹å˜å¤šå°‘
        #
        # Adam å†…éƒ¨åœ¨åšä»€ä¹ˆï¼š
        # m = beta_1 * m_prev + (1 - beta_1) * g       # æ¢¯åº¦çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡
        # v = beta_2 * v_prev + (1 - beta_2) * gÂ²      # æ¢¯åº¦å¹³æ–¹çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡
        # param = param - lr * m / (sqrt(v) + eps)     # æ›´æ–°å‚æ•°
        
        # "running_grad" çš„ä½œç”¨ï¼š
        # running_grad = gradients  # è®°å½•å½“å‰æ¢¯åº¦
        # ç”¨æ¥è®¡ç®—ä¸å‰ä¸€æ­¥æ¢¯åº¦çš„ç›¸ä¼¼åº¦ï¼ˆç”¨äºè¯Šæ–­ï¼‰
```

### ç¬¬ 5 å±‚ï¼šæ—¥å¿—è®°å½•

```python
# æ¯ä¸ª update å‘¨æœŸç»“æŸåï¼š

metrics = {}
metrics['update_step'] = update_step
metrics['return'] = episode_return.mean()      # å¹³å‡å¾—åˆ†
metrics['episode_length'] = episode_len.mean() # å¹³å‡é•¿åº¦
metrics['value_loss'] = value_loss
metrics['actor_loss'] = actor_loss
metrics['entropy'] = entropy
metrics['grad_norm'] = norm(gradients)         # æ¢¯åº¦èŒƒæ•°
metrics['mu_norm'] = norm(adam_m)              # Adam ä¸­çš„ m å‘é‡çš„èŒƒæ•°
metrics['nu_norm'] = norm(adam_v)              # Adam ä¸­çš„ v å‘é‡çš„èŒƒæ•°
metrics['cosine_similarity'] = cos_sim(grad, prev_grad)  # æ¢¯åº¦ç›¸ä¼¼åº¦

# é€šè¿‡ Wandb å‘é€åˆ°è¿œç¨‹æœåŠ¡å™¨
LOGGER.log(exp_id, metrics)
```

---

## PPO ç®—æ³•æ•°å­¦åŸç†ä¸ä»£ç å¯¹åº”

### PPO æ ¸å¿ƒå…¬å¼è¯¦è§£

#### å…¬å¼ 1ï¼šç­–ç•¥æ¯”ç‡ (Policy Ratio)

$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$$

**ä»£ç å¯¹åº”ï¼š**
```python
ratio = jnp.exp(log_prob_new - log_prob_old)
# exp(log(new/old)) = new/old
# è¿™æ ·æ•°å€¼æ›´ç¨³å®šï¼ˆé¿å…é™¤ä»¥å¾ˆå°çš„æ•°ï¼‰
```

#### å…¬å¼ 2ï¼šæ— è£å‰ªç›®æ ‡ (Unclipped Objective)

$$L^{\text{CPI}}(\theta) = \hat{\mathbb{E}}_t \left[ r_t(\theta) \hat{A}_t \right]$$

**ä»£ç å¯¹åº”ï¼š**
```python
loss_actor_unclipped = ratio * gae_minibatch
```

#### å…¬å¼ 3ï¼šè£å‰ªç›®æ ‡ (Clipped Objective) - PPO çš„æ ¸å¿ƒ

$$L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t) \right]$$

**ä»£ç å¯¹åº”ï¼š**
```python
loss_actor_1 = ratio * gae_minibatch  # æ— è£å‰ªç‰ˆæœ¬
loss_actor_2 = jnp.clip(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * gae_minibatch  # è£å‰ªç‰ˆæœ¬
loss_actor = -jnp.minimum(loss_actor_1, loss_actor_2).mean()  # å–è¾ƒå°å€¼
```

**ç›´è§‚ç†è§£ï¼š**

```
åœºæ™¯ 1ï¼šä¼˜åŠ¿ä¸ºæ­£ï¼ˆåšå¾—å¥½ï¼‰
  ratio < 1.0:  loss = ratio Ã— positive  (ä¼šå¢åŠ è¯¥åŠ¨ä½œæ¦‚ç‡)
  ratio >= 1.0: loss = clip(ratio) Ã— positive  (é˜²æ­¢å¢åŠ å¤ªå¤š)

åœºæ™¯ 2ï¼šä¼˜åŠ¿ä¸ºè´Ÿï¼ˆåšå¾—å·®ï¼‰
  ratio < 1.0: loss = clip(ratio) Ã— negative  (ä¼šå‡å°‘è¯¥åŠ¨ä½œæ¦‚ç‡ï¼Œè¢«è£å‰ªä¿æŠ¤)
  ratio > 1.0:  loss = ratio Ã— negative  (ä¸è¢«è£å‰ªï¼Œç»§ç»­å‡å°‘)

ç»“æœï¼šç®—æ³•ä¸ä¼šåšå‡ºå¤ªæç«¯çš„ç­–ç•¥æ”¹å˜ï¼è¿™å°±æ˜¯ PPO ç›¸æ¯”äºå…¶ä»–æ–¹æ³•æ›´ç¨³å®šçš„åŸå› ã€‚
```

#### å…¬å¼ 4ï¼šæ€»æŸå¤±å‡½æ•°

$$L_t^{\text{TOTAL}}(\theta) = L_t^{\text{CLIP}}(\theta) + c_1 L_t^{\text{VF}}(\theta) - c_2 S_t(\theta)$$

**ä»£ç å¯¹åº”ï¼š**
```python
total_loss = (
    loss_actor                          # L_CLIPï¼šç­–ç•¥æŸå¤±
    + config["vf_coef"] * value_loss    # Critic æŸå¤±ï¼ˆc1=0.5ï¼‰
    - config["ent_coef"] * entropy      # ç†µå¥–åŠ±ï¼ˆc2=0.003ï¼‰
)
```

---

## æ”¹è¿›æ–¹å‘æŒ‡å—

### ğŸ”§ æ”¹è¿›æ–¹å‘ 1ï¼šè¶…å‚æ•°è°ƒä¼˜ï¼ˆæœ€ç®€å•ï¼‰

**ä½ æƒ³æ”¹è¿›ä»€ä¹ˆï¼š** æ‰¾åˆ°æœ€ä¼˜çš„ `beta_1` å€¼ï¼ˆæˆ–å…¶ä»–è¶…å‚æ•°ï¼‰

**æ”¹è¿›æ­¥éª¤ï¼š**

1. **ä¿®æ”¹ `sweep_betas.sh`ï¼š**
```bash
#!/bin/bash
# æµ‹è¯•ä¸åŒçš„å­¦ä¹ ç‡
for lr in 1e-3 5e-4 2e-4; do
    echo "è¿è¡Œå­¦ä¹ ç‡ï¼š$lr"
    python ppo_discrete.py lr=$lr
    sleep 10
done
```

2. **åœ¨ Wandb ä¸Šæ¯”è¾ƒç»“æœ**
   - çœ‹æœ€ç»ˆå¾—åˆ†
   - çœ‹æ”¶æ•›é€Ÿåº¦
   - çœ‹è®­ç»ƒçš„ç¨³å®šæ€§

### ğŸ”§ æ”¹è¿›æ–¹å‘ 2ï¼šæ”¹è¿›ç½‘ç»œç»“æ„ï¼ˆä¸­ç­‰éš¾åº¦ï¼‰

**ä½ æƒ³æ”¹è¿›ä»€ä¹ˆï¼š** è®© AI"å¤§è„‘"æ›´å¼ºå¤§

**æ”¹è¿›æ­¥éª¤ï¼š**

ä¿®æ”¹ `networks/mlp.py`ï¼š

```python
# åŸä»£ç ï¼šä¸¤å±‚ 64 å•å…ƒ
# æ–°ä»£ç ï¼šä¸‰å±‚æ›´å¤§çš„ç½‘ç»œ
class ActorCriticDiscrete(nn.Module):
    action_dim: Sequence[int]
    activation: str = "tanh"

    @nn.compact
    def __call__(self, x):
        # æ”¹è¿› 1ï¼šåŠ æ·±ç½‘ç»œ
        hidden_dim = 256  # æ–°å¢é…ç½®é¡¹
        
        if self.activation == "relu":
            activation = nn.relu
        else:
            activation = nn.tanh
        
        # Actor åˆ†æ”¯
        actor_mean = nn.Dense(hidden_dim, ...)(x)
        actor_mean = activation(actor_mean)
        actor_mean = nn.Dense(hidden_dim, ...)(actor_mean)
        actor_mean = activation(actor_mean)
        actor_mean = nn.Dense(hidden_dim // 2, ...)(actor_mean)  # æ–°å±‚
        actor_mean = activation(actor_mean)
        actor_mean = nn.Dense(self.action_dim, ...)(actor_mean)
        pi = Categorical(logits=actor_mean)
        
        # Critic åˆ†æ”¯ï¼ˆç›¸åŒä¿®æ”¹ï¼‰
        ...
```

### ğŸ”§ æ”¹è¿›æ–¹å‘ 3ï¼šæ”¹è¿›ç®—æ³•ï¼ˆé«˜éš¾åº¦ä½†æœ€æœ‰ä»·å€¼ï¼‰

**ä½ æƒ³æ”¹è¿›ä»€ä¹ˆï¼š** æ”¹è¿› PPO ç®—æ³•æœ¬èº«çš„æ ¸å¿ƒ

#### é€‰é¡¹ Aï¼šå®ç°"EMA Running Gradient"

å½“å‰ä»£ç ï¼š
```python
new_running_grad = grads  # ç›´æ¥æ›¿æ¢
```

æ”¹è¿›ç‰ˆæœ¬ï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰ï¼š
```python
ema_decay = 0.99
new_running_grad = jax.tree.map(
    lambda rg, g: rg * ema_decay + g * (1 - ema_decay),
    running_grad,
    grads
)

# è¿™æ ·å¯ä»¥çœ‹æ¢¯åº¦æ–¹å‘çš„é•¿æœŸå˜åŒ–è¶‹åŠ¿
cos_sim = cosine_similarity(grads, new_running_grad)
# å¦‚æœ cos_sim æ¥è¿‘ 1ï¼šæ¢¯åº¦æ–¹å‘ç¨³å®šï¼ˆå¥½äº‹ï¼‰
# å¦‚æœ cos_sim æ¥è¿‘ 0ï¼šæ¢¯åº¦æ–¹å‘å˜åŒ–å¤§ï¼ˆå¯èƒ½æœ‰é—®é¢˜ï¼‰
```

#### é€‰é¡¹ Bï¼šåŠ å…¥"å¥–åŠ±æ•´å½¢"

æƒ³è±¡ AI å­¦ä¼šäº†"ä½œå¼Š"ã€‚ä¾‹å¦‚åœ¨å±±åœ°è½¦æ¸¸æˆä¸­ï¼Œä¸æ˜¯çœŸçš„ä¸Šå±±ï¼Œè€Œæ˜¯æ‰¾åˆ°æŸç§æ–¹å¼è·å¾—è™šå‡é«˜åˆ†ã€‚

æ”¹è¿›æ–¹æ³•ï¼š
```python
# åœ¨ reward è®¡ç®—ä¸­åŠ å…¥é¢å¤–çš„ä¿¡æ¯
# æ¯”å¦‚é¼“åŠ±"å‘å‰"è¿™ä¸ªåŠ¨ä½œæœ¬èº«
shaped_reward = reward + 0.01 * (position_change)
```

#### é€‰é¡¹ Cï¼šå®ç°"å¹¿ä¹‰ä¼˜åŠ¿æ ‡å‡†åŒ–"

å½“å‰ä»£ç ï¼š
```python
gae_minibatch = (gae_minibatch - gae_minibatch.mean()) / (gae_minibatch.std() + 1e-8)
```

æ”¹è¿›ç‰ˆæœ¬ï¼ˆè·¨ minibatch æ ‡å‡†åŒ–ï¼‰ï¼š
```python
# è®°å½•æ‰€æœ‰ minibatch çš„ GAEï¼Œç„¶åç»Ÿä¸€æ ‡å‡†åŒ–
all_gae = jnp.concatenate([gae for all minibatches])
global_mean = all_gae.mean()
global_std = all_gae.std()

# è¿™æ ·å¯ä»¥é¿å…æŸäº› minibatch å› ä¸ºç‰¹æ®Šæƒ…å†µå¯¼è‡´è¿‡åº¦æ ‡å‡†åŒ–
gae_normalized = (gae - global_mean) / (global_std + 1e-8)
```

---

## ğŸ¯ å­¦ä¹ è·¯çº¿å›¾

**ç¬¬ 1 å¤©ï¼š** ç†è§£è¿™ä»½æ–‡æ¡£ï¼Œè¿è¡Œä¸€æ¬¡å®Œæ•´è®­ç»ƒ

```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆ5 åˆ†é’Ÿï¼‰
python ppo_discrete.py total_timesteps=10000 num_seeds=1 num_envs=4
```

**ç¬¬ 2-3 å¤©ï¼š** ä½¿ç”¨ `jax.debug.print` æ£€æŸ¥ä¸­é—´å˜é‡

åœ¨ `_loss` å‡½æ•°ä¸­æ·»åŠ ï¼š
```python
jax.debug.print("ratio mean: {x}", x=jnp.mean(ratio))
jax.debug.print("advantage mean: {x}", x=jnp.mean(gae_minibatch))
jax.debug.print("entropy: {x}", x=entropy)
```

**ç¬¬ 4-5 å¤©ï¼š** åšç¬¬ä¸€ä¸ªæ”¹è¿›å®éªŒï¼ˆè¶…å‚æ•°è°ƒä¼˜ï¼‰

```bash
# åˆ›å»º sweep è„šæœ¬
bash sweep_learning_rates.sh
```

**ç¬¬ 6-7 å¤©ï¼š** æ·±å…¥ç ”ç©¶ä½ æ„Ÿå…´è¶£çš„æ”¹è¿›æ–¹å‘

é€‰æ‹©ä¸Šé¢æåˆ°çš„ 3 ä¸ªæ”¹è¿›æ–¹å‘ä¹‹ä¸€ï¼Œå®ç°å¹¶æµ‹è¯•ã€‚

---

## ğŸ“Š å…³é”®æŒ‡æ ‡è§£è¯»

| æŒ‡æ ‡ | å«ä¹‰ | å¥½çš„å€¼ | åçš„å€¼ |
|------|------|--------|-------|
| `return` | å¹³å‡æ¯æ¡æ¸¸æˆçš„å¾—åˆ† | â†—ï¸ è¶‹åŠ¿å‘ä¸Š | â†˜ï¸ è¶‹åŠ¿å‘ä¸‹ |
| `actor_loss` | ç­–ç•¥æŸå¤± | ç¨³å®šæˆ–ä¸‹é™ | å¿«é€Ÿä¸Šå‡ |
| `value_loss` | Critic æŸå¤± | ç¨³å®šæˆ–ä¸‹é™ | å¿«é€Ÿä¸Šå‡ |
| `entropy` | ç­–ç•¥éšæœºæ€§ | ç¼“æ…¢ä¸‹é™ | å¿«é€Ÿä¸‹é™æˆ–å¿«é€Ÿä¸Šå‡ |
| `grad_norm` | æ¢¯åº¦èŒƒæ•° | æ¥è¿‘ 1.0 | è¿œå¤§äº 1 æˆ–æ¥è¿‘ 0 |
| `cosine_similarity` | æ¢¯åº¦ç›¸ä¼¼åº¦ | 0.7-0.9 | < 0.5ï¼ˆä¸ç¨³å®šï¼‰ |
| `clip_frac` | è¢«è£å‰ªçš„æ¯”ä¾‹ | 0.1-0.2 | > 0.5ï¼ˆå¤ªå¤šè£å‰ªï¼‰ |

---

## âš ï¸ å¸¸è§é”™è¯¯ä¸è°ƒè¯•

### é”™è¯¯ 1ï¼š`PYTHONPATH` æ‰¾ä¸åˆ°æ¨¡å—

```bash
# âŒ é”™è¯¯
python ppo_discrete.py

# âœ… æ­£ç¡®
PYTHONPATH=/home/yichen/ADAM/optimize python3 ppo_discrete.py
```

### é”™è¯¯ 2ï¼š`conda activate` åœ¨è„šæœ¬ä¸­ä¸å·¥ä½œ

```bash
# âŒ è„šæœ¬ä¸­
conda activate adam

# âœ… æ”¹æˆ
source ~/miniconda3/etc/profile.d/conda.sh && conda activate adam

# æˆ–è€…
conda run -n adam python ppo_discrete.py
```

### é”™è¯¯ 3ï¼šæŸå¤±ä¸€ç›´ä¸Šå‡

**å¯èƒ½åŸå› **ï¼š
- å­¦ä¹ ç‡å¤ªå¤§ â†’ æ”¹å° lr
- æ¢¯åº¦çˆ†ç‚¸ â†’ æ£€æŸ¥ grad_normï¼Œå¯èƒ½éœ€è¦å‡å°ç½‘ç»œå¤§å°

### é”™è¯¯ 4ï¼šè®­ç»ƒä¸€ç›´ä¸æ”¶æ•›

**å¯èƒ½åŸå› **ï¼š
- å­¦ä¹ ç‡å¤ªå° â†’ æ”¹å¤§ lr
- ç†µç³»æ•°å¤ªå¤§ â†’ å‡å° ent_coef
- ç¯å¢ƒéš¾åº¦å¤ªé«˜ â†’ æ”¹ç”¨ç®€å•ç¯å¢ƒæµ‹è¯•

---

## æ€»ç»“ï¼š100% ç†è§£æ£€æŸ¥æ¸…å•

å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼Œä½ å°±è¾¾åˆ°äº† 100% ç†è§£ï¼š

- [ ] èƒ½å¤Ÿè§£é‡Šæ¯ä¸ªè¶…å‚æ•°çš„ä½œç”¨å’Œä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè®¾ç½®
- [ ] èƒ½å¤Ÿç”»å‡ºæ•´ä¸ªè®­ç»ƒå¾ªç¯çš„æµç¨‹å›¾
- [ ] èƒ½å¤ŸæŒ‡å‡ºä»£ç ä¸­æ¯ä¸€è¡Œå¯¹åº” PPO è®ºæ–‡çš„å“ªä¸€éƒ¨åˆ†
- [ ] èƒ½å¤Ÿè¿è¡Œä¸€æ¬¡å®Œæ•´è®­ç»ƒå¹¶ç†è§£ Wandb è¾“å‡ºçš„æ¯ä¸ªå›¾è¡¨
- [ ] èƒ½å¤Ÿä¿®æ”¹ä¸€ä¸ªè¶…å‚æ•°ï¼Œé¢„æµ‹ä¼šå¯¹è®­ç»ƒäº§ç”Ÿä»€ä¹ˆå½±å“
- [ ] èƒ½å¤Ÿå¢åŠ ä¸€ä¸ª `jax.debug.print` è°ƒè¯•è¯­å¥ï¼Œå¹¶ç†è§£è¾“å‡º
- [ ] èƒ½å¤Ÿå®ç°ä¸Šé¢æåˆ°çš„è‡³å°‘ä¸€ä¸ªæ”¹è¿›æ–¹å‘
- [ ] èƒ½å¤Ÿå¯¹æ¯”ä¸¤ä¸ªå®éªŒï¼Œè¯´å‡ºå·®å¼‚çš„åŸå› 

---

**ç»§ç»­åŠ æ²¹ï¼ğŸš€**
